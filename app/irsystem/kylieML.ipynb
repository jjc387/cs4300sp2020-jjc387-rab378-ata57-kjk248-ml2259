{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('winereviews.csv')\n",
    "#data.head()\n",
    "column_headers = [\"country\", \"description\",\"points\", \"province\", \"region_1\", \"variety\", \"winery\"]\n",
    "data = pd.read_csv('winereviews.csv', usecols=column_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.winery.notnull()]\n",
    "data = data[data.variety.notnull()]\n",
    "data = data[data.province != \"Other\"]\n",
    "data = data[data.province.notnull()]\n",
    "data = data[data.province != \"France Other\"]\n",
    "data = data[data.province != \"Spain Other\"]\n",
    "data = data[data.province != \"Australia Other\"]\n",
    "data = data[data.country != \"US-France\"]\n",
    "data = data[data.points > 88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (4.0.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from gensim) (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(data['description'])\n",
    "reviews = [str(r) for r in reviews]\n",
    "reviews_full_doc = ' '.join(reviews)\n",
    "sent_tokens = sent_tokenize(reviews_full_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words, lowercase, and punctuation\n",
    "punc = '''!()-[]{};:.'\"\\, <>./?@#$%^&*_~'''\n",
    "\n",
    "def process_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    processed_sentence = []\n",
    "    for word in words:\n",
    "        w = str(word)\n",
    "        lowercase = str.lower(w)\n",
    "        stemmed = SnowballStemmer('english').stem(lowercase)\n",
    "        remove_punc = stemmed\n",
    "        for c in remove_punc:\n",
    "            if c in punc:\n",
    "                remove_punc.replace(c, \"\")\n",
    "        processed_sentence.append(remove_punc)\n",
    "    return(processed_sentence)\n",
    "        \n",
    "processed_sents = []\n",
    "for sent in sent_tokens:\n",
    "    processed = process_text(sent)\n",
    "    processed_sents.append(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_csv('descriptor_mapping.csv').set_index('raw descriptor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bi grams/phrases\n",
    "phrases = Phrases(processed_sents)\n",
    "phrases = Phrases(phrases[processed_sents])\n",
    "grams = Phraser(phrases)\n",
    "phrase_sents = []\n",
    "for sent in processed_sents:\n",
    "    phrased = grams[sent]\n",
    "    phrase_sents.append(phrased)\n",
    "    \n",
    "allwords = [item for sublist in phrase_sents for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map corpus words to level 3\n",
    "def mapped_words(word):\n",
    "    if word in list(mapping.index):\n",
    "        mapped = mapping['level_3'][word]\n",
    "        return mapped\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "processed_sents = []\n",
    "for sent in phrase_sents:\n",
    "    processed_sent = []\n",
    "    for word in sent:\n",
    "        mapped_word = mapped_words(word)\n",
    "        processed_sent.append(str(mapped_word))\n",
    "    processed_sents.append(processed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(processed_sents, vector_size=300, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nectarine', 0.8802266120910645),\n",
       " ('pear', 0.8734254240989685),\n",
       " ('mango', 0.8442874550819397),\n",
       " ('tangerine', 0.8379481434822083),\n",
       " ('melon', 0.8357724547386169),\n",
       " ('quince', 0.8332266807556152),\n",
       " ('honeysuckle', 0.8276798129081726),\n",
       " ('pineapple', 0.8143859505653381),\n",
       " ('baked_apple', 0.8120083808898926),\n",
       " ('honeydew', 0.8113285303115845)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#could use for query expansion\n",
    "model.wv.most_similar(positive='peach', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '''!()-[]{};:.'\"\\, <>./?@#$%^&*_~'''\n",
    "all_des_words = list(set(all_des_words))\n",
    "for i in range(len(all_des_words)):\n",
    "    word = all_des_words[i]\n",
    "    if word[len(word) - 1] in punc:\n",
    "        remv = word[:len(word) -1]\n",
    "        all_des_words[i] = remv\n",
    "#all_des_words[2][len(all_des_words[2]) -1] in punc\n",
    "#len(all_des_words[2])\n",
    "\n",
    "for i in range(len(all_des_words)):\n",
    "    word = all_des_words[i]\n",
    "    all_des_words[i] = word.lower()\n",
    "\n",
    "all_des_words = set(all_des_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in reviews:\n",
    "    processed = process_text(review)\n",
    "    phrased = grams[processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update review data to only show meaningful words\n",
    "reviews = list(data['description'])\n",
    "mapped_reviews = []\n",
    "def mapping_wine_words(word):\n",
    "    if word in list(mapping.index):\n",
    "        mapped = mapping['level_3'][word]\n",
    "        return mapped\n",
    "for review in reviews:\n",
    "    processed = process_text(review)\n",
    "    phrased = grams[processed]\n",
    "    mapped = [mapping_wine_words(word) for word in phrased]\n",
    "    remove_none = [str(d) for d in mapped if d is not None]\n",
    "    mapped_rev = ' '.join(remove_none)\n",
    "    mapped_reviews.append(mapped_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create embedding vectors for each review\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit(mapped_reviews)\n",
    "dict_of_tfidf_weightings = dict(zip(X.get_feature_names(), X.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_vectors = []\n",
    "for review in mapped_reviews:\n",
    "    numwords = 0\n",
    "    words = review.split(' ')\n",
    "    tfidf_mapped_review = []\n",
    "    for word in words:\n",
    "        if word in dict_of_tfidf_weightings.keys():\n",
    "            tfidf = dict_of_tfidf_weightings[word]\n",
    "            vec = tfidf * model.wv.get_vector(word).reshape(1, 300)\n",
    "            tfidf_mapped_review.append(vec)\n",
    "            numwords = numwords + 1\n",
    "    if len(tfidf_mapped_review) > 0:\n",
    "       # weight_rev_vec = sum(tfidf_mapped_review)/len(tfidf_mapped_review)\n",
    "        weight_rev_vec = sum(tfidf_mapped_review)\n",
    "    else:\n",
    "        weight_rev_vec = []\n",
    "    review_vectors.append(weight_rev_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vectors_list = []\n",
    "for vec in review_vectors:\n",
    "    if len(vec) == 1:\n",
    "        vectors_list.append(np.float32(vec[0]))\n",
    "    else:\n",
    "        vectors_list.append(np.float32(np.zeros(300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "knn = NearestNeighbors(n_neighbors=10, algorithm= 'brute', metric='cosine')\n",
    "modelknn = knn.fit(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['peach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightedqueryterms = []\n",
    "for term in query:\n",
    "    if term in dict_of_tfidf_weightings:\n",
    "        tfidfweight = dict_of_tfidf_weightings[term]\n",
    "        word_vector = tfidfweight * model.wv.get_vector(term).reshape(1,300)\n",
    "        weightedqueryterms.append(word_vector)\n",
    "    else:\n",
    "        try:\n",
    "            #print(term)\n",
    "            word_vector = model.wv.get_vector(term)\n",
    "            word_vector.shape(1,300)\n",
    "            #print(word_vector)\n",
    "            weightedqueryterms.append(word_vector)\n",
    "        except:\n",
    "            print(\"???\")\n",
    "query_vec = sum(weightedqueryterms)\n",
    "#print(query_vec)\n",
    "distance, indice = modelknn.kneighbors(query_vec, n_neighbors = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_list = distance[0].tolist()[1:]\n",
    "indice_list = indice[0].tolist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in indice_list:\n",
    "#     print(i)\n",
    "#     review = mapped_reviews[i]\n",
    "#     location = data['province'][i]\n",
    "#     wine = data['variety'][i]\n",
    "#     print(location)\n",
    "#     print(wine)\n",
    "#     print(review)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new word embedding tfidf matrix of reviews\n",
    "#np.array(vectors_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = list(data['description'])\n",
    "data['full_review'] = full\n",
    "data['description'] = mapped_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dict = data.to_dict(orient='index')\n",
    "#wine_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27596, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_word2vec = []\n",
    "words_word2vec_dict = {}\n",
    "for i, key in enumerate(list(model.wv.index_to_key)):\n",
    "    words_word2vec_dict[key] = i\n",
    "    embedding = model.wv.get_vector(key, norm=True)\n",
    "    matrix_word2vec.append(embedding)\n",
    "matrix_word2vec = np.array(matrix_word2vec)\n",
    "matrix_word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_word2vec = sparse.csr_matrix(matrix_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_list = np.array(vectors_list)\n",
    "vectors_list_sparse = sparse.csr_matrix(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_idx = {}\n",
    "for idx in wine_dict:\n",
    "    c = wine_dict[idx]['country']\n",
    "    if c not in country_to_idx:\n",
    "        country_to_idx[c] = []\n",
    "    country_to_idx[c].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MAIN PICKLES ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('wine_dict02.pickle', 'wb') as handle:\n",
    "#      pickle.dump(wine_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('review_tfidf_embeddings01.pickle', 'wb') as handle:\n",
    "#      pickle.dump(vectors_list, handle, protocol=pickle.HIGHEST_PROTOCOL)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('idf_weight_dict0.pickle', 'wb') as handle:\n",
    "#      pickle.dump(dict_of_tfidf_weightings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('matrix_word2vec0.pickle', 'wb') as handle:\n",
    "#      pickle.dump(matrix_word2vec, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('country_to_idx0.pickle', 'wb') as handle:\n",
    "#      pickle.dump(country_to_idx, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('words_word2vec_dict0.pickle', 'wb') as handle:\n",
    "#     pickle.dump(words_word2vec_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('wine_descript_dict.pickle', 'wb') as handle:\n",
    "#     pickle.dump(wine_descript_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
